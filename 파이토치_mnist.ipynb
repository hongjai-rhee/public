{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "파이토치_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM101nSqHfslgRaSGB5iu8N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hongjai-rhee/public/blob/master/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ycRe38WLKkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "from __future__ import print_function; from torch import nn, optim, cuda\n",
        "from torch.utils import data; from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F ; import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvm28HsSLjEQ",
        "colab_type": "text"
      },
      "source": [
        "# 런타임유형에서 GPU로 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2UyM-1RPCGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3f38c912-600a-4364-d52c-3987178babe1"
      },
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MNIST Model on cuda\n",
            "============================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mia39tY2LV9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "EPOCH=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyQC3e2NPNuI",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP2drFgPLo7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# MNIST Dataset을 텐서로 변환\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())    \n",
        "\n",
        "# Data Loader (배치 사이즈에 맞도록 데이터 정리)\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = data.DataLoader(dataset=test_dataset,batch_size=batch_size, shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v61bc7QPL-_I",
        "colab_type": "text"
      },
      "source": [
        "# 모형설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WhSrzLmL3Ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):    ## 레이어 구조 설정 (뉴론의 수)\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(784, 520)   ## 인풋레이어        \n",
        "        self.l2 = nn.Linear(520, 320)   ## 히든 1\n",
        "        self.l3 = nn.Linear(320, 240)   ## 히든 2\n",
        "        self.l4 = nn.Linear(240, 120)   ## 히든 3 \n",
        "        self.l5 = nn.Linear(120, 10)    ## 아웃풋레이어\n",
        "\n",
        "    def forward(self, x):  ## 연산방법과 활성화함수 설정\n",
        "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))\n",
        "        return self.l5(x)               ## no softmax here\n",
        "\n",
        "model = Net()\n",
        "model.to(device)   ## GPU 지정\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()    ## 손실함수 지정\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)   ## 최적화모듈 지정\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FiA3VanMftf",
        "colab_type": "text"
      },
      "source": [
        "# 학습과 검증 모듈 (그대로 두면 됨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU-bNUIRMQ9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urtUihI2Mu4Q",
        "colab_type": "text"
      },
      "source": [
        "# 학습 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFDOFSsGMmpv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01527cca-702c-4a08-80bd-1fd38df11f46"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCH):\n",
        "      train(epoch)\n",
        "      test()\n",
        "     "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.302506\n",
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.307787\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.308123\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.301888\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.306642\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.302096\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.296173\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.301668\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.306900\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.297768\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.305807\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.310205\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.310447\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.304529\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.297220\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.301138\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.303030\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.296864\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.291097\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.296142\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.297399\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.290540\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.302495\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.302295\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.292876\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.292164\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.297797\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.293932\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.290638\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.295155\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.298632\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.293125\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.295868\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.295651\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.292987\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.282532\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.289411\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.295430\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.286436\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.296626\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.281440\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.291924\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.283359\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.287860\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.289778\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.284105\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.287013\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.292155\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.293375\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.280778\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.279830\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.284797\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.277049\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.270085\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.275862\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.269977\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.274374\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.264448\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.261599\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.266421\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.272379\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.274761\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.261834\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.247093\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.262951\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.264007\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.236259\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.244761\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.245242\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.227509\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.240109\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.221377\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.211243\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.217358\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.192093\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.184639\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.191285\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.161390\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.139108\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 2.104134\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.135611\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 2.134908\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 2.071697\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 2.042699\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 2.025701\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 2.003729\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.944849\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 2.017808\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.941017\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.933663\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.996152\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.799888\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 1.718548\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.847452\n",
            "===========================\n",
            "Test set: Average loss: 0.0272, Accuracy: 3816/10000 (38%)\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.734902\n",
            "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 1.762833\n",
            "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.717039\n",
            "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.636211\n",
            "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 1.521079\n",
            "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 1.410039\n",
            "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 1.549928\n",
            "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 1.527501\n",
            "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 1.525354\n",
            "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 1.408709\n",
            "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 1.411662\n",
            "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 1.274104\n",
            "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 1.361720\n",
            "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 1.218282\n",
            "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 1.186993\n",
            "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 1.187461\n",
            "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 1.098868\n",
            "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 1.272178\n",
            "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 1.230674\n",
            "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.980273\n",
            "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 1.084093\n",
            "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 1.223429\n",
            "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.855031\n",
            "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.959441\n",
            "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.939257\n",
            "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.838432\n",
            "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.809424\n",
            "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.721923\n",
            "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.799358\n",
            "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.855748\n",
            "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.781323\n",
            "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.767489\n",
            "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.831281\n",
            "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.659557\n",
            "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.644019\n",
            "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.863068\n",
            "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.631292\n",
            "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.693089\n",
            "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.978082\n",
            "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.670793\n",
            "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.648642\n",
            "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.551091\n",
            "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.805752\n",
            "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.523101\n",
            "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.594625\n",
            "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.814882\n",
            "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.594529\n",
            "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.597339\n",
            "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.700938\n",
            "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.725333\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.481526\n",
            "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.589630\n",
            "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.676768\n",
            "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.571167\n",
            "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.759927\n",
            "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.490024\n",
            "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.386290\n",
            "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.635352\n",
            "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.907982\n",
            "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.740187\n",
            "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.834841\n",
            "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.611378\n",
            "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.566354\n",
            "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.841890\n",
            "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.529786\n",
            "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.466388\n",
            "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.562430\n",
            "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.532413\n",
            "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.533447\n",
            "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.571133\n",
            "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.665893\n",
            "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.453213\n",
            "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.471360\n",
            "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.700354\n",
            "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.449390\n",
            "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.417594\n",
            "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.385259\n",
            "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.447567\n",
            "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.570420\n",
            "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.490789\n",
            "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.367538\n",
            "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.428573\n",
            "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.629089\n",
            "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.377586\n",
            "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.354845\n",
            "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.718800\n",
            "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.602009\n",
            "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.596682\n",
            "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.310371\n",
            "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.455635\n",
            "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.406042\n",
            "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.527714\n",
            "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.393058\n",
            "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.600065\n",
            "===========================\n",
            "Test set: Average loss: 0.0072, Accuracy: 8610/10000 (86%)\n",
            "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.335874\n",
            "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.689697\n",
            "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.450930\n",
            "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.341248\n",
            "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.578424\n",
            "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.435694\n",
            "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.429156\n",
            "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.586239\n",
            "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.389458\n",
            "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.352275\n",
            "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.307341\n",
            "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.518263\n",
            "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.294019\n",
            "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.565357\n",
            "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.698693\n",
            "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.386172\n",
            "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.319450\n",
            "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.442198\n",
            "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.280653\n",
            "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.547287\n",
            "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.368599\n",
            "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.393964\n",
            "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.418383\n",
            "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.279022\n",
            "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.366752\n",
            "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.554733\n",
            "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.305793\n",
            "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.514907\n",
            "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.222099\n",
            "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.449801\n",
            "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.430953\n",
            "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.304229\n",
            "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.225628\n",
            "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.342715\n",
            "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.481663\n",
            "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.440411\n",
            "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.425178\n",
            "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.725186\n",
            "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.390438\n",
            "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.329830\n",
            "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.530873\n",
            "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.254446\n",
            "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.332215\n",
            "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.299983\n",
            "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.180275\n",
            "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.283188\n",
            "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.312182\n",
            "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.363944\n",
            "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.220095\n",
            "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.414648\n",
            "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.246515\n",
            "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.464707\n",
            "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.360673\n",
            "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.385543\n",
            "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.476339\n",
            "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.425455\n",
            "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.260506\n",
            "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.313278\n",
            "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.304552\n",
            "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.359809\n",
            "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.243325\n",
            "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.279031\n",
            "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.290644\n",
            "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.263576\n",
            "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.245299\n",
            "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.434585\n",
            "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.324692\n",
            "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.353618\n",
            "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.300913\n",
            "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.311554\n",
            "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.327666\n",
            "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.286179\n",
            "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.259637\n",
            "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.679746\n",
            "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.490640\n",
            "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.264501\n",
            "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.139445\n",
            "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.444003\n",
            "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.365228\n",
            "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.301612\n",
            "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.502517\n",
            "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.294464\n",
            "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.337937\n",
            "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.463822\n",
            "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.229907\n",
            "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.332833\n",
            "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.260135\n",
            "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.301588\n",
            "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.252980\n",
            "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.398943\n",
            "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.439448\n",
            "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.439243\n",
            "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.322978\n",
            "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.394620\n",
            "===========================\n",
            "Test set: Average loss: 0.0050, Accuracy: 9077/10000 (91%)\n",
            "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.414568\n",
            "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.278658\n",
            "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.410564\n",
            "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.184301\n",
            "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.427102\n",
            "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.375159\n",
            "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.251328\n",
            "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.256120\n",
            "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.349192\n",
            "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.515075\n",
            "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.274435\n",
            "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.347865\n",
            "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.217701\n",
            "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.557147\n",
            "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.231695\n",
            "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.164843\n",
            "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.231748\n",
            "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.281672\n",
            "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.302255\n",
            "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.374367\n",
            "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.536110\n",
            "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.221086\n",
            "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.302238\n",
            "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.094122\n",
            "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.144926\n",
            "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.603726\n",
            "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.247853\n",
            "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.156389\n",
            "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.232655\n",
            "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.178820\n",
            "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.355940\n",
            "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.333702\n",
            "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.351059\n",
            "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.297934\n",
            "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.152804\n",
            "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.235472\n",
            "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.354949\n",
            "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.200994\n",
            "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.232729\n",
            "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.177757\n",
            "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.353644\n",
            "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.289437\n",
            "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.552757\n",
            "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.296963\n",
            "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.224330\n",
            "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.307132\n",
            "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.275100\n",
            "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.147294\n",
            "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.269560\n",
            "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.184327\n",
            "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.345710\n",
            "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.388898\n",
            "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.279278\n",
            "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.401806\n",
            "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.251274\n",
            "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.418026\n",
            "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.165553\n",
            "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.368244\n",
            "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.349360\n",
            "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.267284\n",
            "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.200819\n",
            "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.287730\n",
            "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.325875\n",
            "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.285542\n",
            "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.276759\n",
            "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.353104\n",
            "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.232021\n",
            "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.313139\n",
            "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.356734\n",
            "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.344191\n",
            "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.250384\n",
            "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.196931\n",
            "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.176189\n",
            "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.352960\n",
            "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.148265\n",
            "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.277084\n",
            "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.373053\n",
            "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.439889\n",
            "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.337742\n",
            "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.158314\n",
            "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.326061\n",
            "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.314097\n",
            "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.209610\n",
            "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.264123\n",
            "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.193841\n",
            "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.249253\n",
            "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.196085\n",
            "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.219688\n",
            "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.255721\n",
            "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.253877\n",
            "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.197826\n",
            "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.208004\n",
            "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.126292\n",
            "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.108261\n",
            "===========================\n",
            "Test set: Average loss: 0.0037, Accuracy: 9289/10000 (93%)\n",
            "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.253860\n",
            "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.256170\n",
            "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.326548\n",
            "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.202181\n",
            "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.314532\n",
            "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.124194\n",
            "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.244221\n",
            "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.375787\n",
            "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.131946\n",
            "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.130375\n",
            "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.220188\n",
            "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.217597\n",
            "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.230395\n",
            "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.396414\n",
            "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.131141\n",
            "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.203005\n",
            "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.217749\n",
            "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.185301\n",
            "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.156066\n",
            "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.155974\n",
            "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.259174\n",
            "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.237250\n",
            "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.177494\n",
            "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.210360\n",
            "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.301449\n",
            "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.292744\n",
            "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.238173\n",
            "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.275348\n",
            "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.188923\n",
            "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.430614\n",
            "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.121412\n",
            "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.119654\n",
            "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.268408\n",
            "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.162644\n",
            "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.116466\n",
            "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.215773\n",
            "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.089376\n",
            "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.316783\n",
            "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.159907\n",
            "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.370557\n",
            "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.150927\n",
            "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.256676\n",
            "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.156290\n",
            "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.086743\n",
            "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.099058\n",
            "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.118372\n",
            "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.366850\n",
            "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.262524\n",
            "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.216076\n",
            "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.117738\n",
            "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.204437\n",
            "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.121744\n",
            "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.313318\n",
            "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.120638\n",
            "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.144319\n",
            "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.216130\n",
            "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.127980\n",
            "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.157491\n",
            "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.148548\n",
            "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.157666\n",
            "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.164103\n",
            "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.186512\n",
            "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.162696\n",
            "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.273599\n",
            "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.146196\n",
            "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.083602\n",
            "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.249449\n",
            "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.137881\n",
            "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.328017\n",
            "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.073624\n",
            "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.076451\n",
            "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.322247\n",
            "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.321009\n",
            "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.247043\n",
            "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.193896\n",
            "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.066215\n",
            "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.471672\n",
            "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.093753\n",
            "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.185689\n",
            "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.084130\n",
            "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.186524\n",
            "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.216543\n",
            "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.100651\n",
            "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.110287\n",
            "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.139162\n",
            "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.187709\n",
            "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.085674\n",
            "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.099938\n",
            "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.134143\n",
            "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.219426\n",
            "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.197601\n",
            "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.264623\n",
            "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.277715\n",
            "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.143741\n",
            "===========================\n",
            "Test set: Average loss: 0.0027, Accuracy: 9504/10000 (95%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}